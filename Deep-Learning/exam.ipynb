{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# examples for different achitectures of neuronal networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Implementation\n",
    "## predicts configuration and assumes that e.g. Carbon atom chains are in 3D space and of length 20 atoms ->(input_dim=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden1): Linear(in_features=60, out_features=64, bias=True)\n",
      "  (hidden2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (hidden3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (hidden4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.in_size = 60       #len(inputs[0,:])\n",
    "        self.hidden1_size = 64\n",
    "        self.hidden2_size = 128\n",
    "        self.hidden3_size = 64\n",
    "        self.hidden4_size = 32\n",
    "        self.out_size = 1\n",
    "        \n",
    "        self.hidden1 = nn.Linear(self.in_size,self.hidden1_size)\n",
    "        self.hidden2 = nn.Linear(self.hidden1_size,self.hidden2_size)\n",
    "        self.hidden3 = nn.Linear(self.hidden2_size,self.hidden3_size)\n",
    "        self.hidden4 = nn.Linear(self.hidden3_size,self.hidden4_size)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden4_size,self.out_size)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        inputs = nn.functional.elu(self.hidden1(inputs))\n",
    "        inputs = nn.functional.elu(self.hidden2(inputs))\n",
    "        inputs = nn.functional.elu(self.hidden3(inputs))\n",
    "        inputs = nn.functional.elu(self.hidden4(inputs))\n",
    "        out = self.out(inputs)\n",
    "        out = out.view(-1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchwise data augmentation \n",
    "### as it wasn't further specified, this implementation requires to concatenate original data and new augemented data afterwards (if wanted) to increase the total data amount for later training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(data, augment_rotation: bool, augment_permutation: bool):\n",
    "    # data is a numpy array with shape (batch_size, 20, 3)\n",
    "    out = data.copy()\n",
    "    batch_size = len(data[:,0,0])\n",
    "    original_list = np.arange(20)\n",
    "    indice_list = np.arange(20)\n",
    "    if augment_rotation:\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            M = np.random.normal(size=(3, 3))\n",
    "            rotation_matrix = np.linalg.qr(M)[0] #rot. matrix creation\n",
    "            for j in range(20):\n",
    "                out[i][j] = rotation_matrix.dot(out[i,j,:]) #overwriting\n",
    "                \n",
    "        \n",
    "    if augment_permutation:\n",
    "        \n",
    "        for n in range(batch_size):\n",
    "            np.random.shuffle(indice_list) # new indice ordering of indice_list\n",
    "            for m in range(20):\n",
    "                original_indice = original_list[m]\n",
    "                out[n][original_indice] = out[n,indice_list[m],:] #overwriting\n",
    "    # return augmented data with shape (batch_size, 60)\n",
    "    out = torch.from_numpy(out.astype(np.float32))\n",
    "    out = out.view(batch_size,60) #providing shape of input that is later required\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some testing on random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 3)\n",
      "torch.Size([10, 60])\n",
      "<class 'torch.Tensor'>\n",
      "Number of Configurations/ One per batch element: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#defining the data for  test:\n",
    "data_np = np.random.rand(10,20,3)\n",
    "print(data_np.shape)\n",
    "# performing data rotation and permutation\n",
    "output_np = prepare_batch(data_np, augment_rotation = True, augment_permutation = True)\n",
    "print(output_np.shape)\n",
    "print(type(output_np))\n",
    "# Applys data to model and receives configurations according\n",
    "# to number of data sets, that were inserted.\n",
    "predics = model(output_np)\n",
    "print('Number of Configurations/ One per batch element:', predics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation invariant architecture F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi(\n",
      "  (lay1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (lay2): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (lay3): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (out): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "Rho(\n",
      "  (lay1): Linear(in_features=100, out_features=132, bias=True)\n",
      "  (lay2): Linear(in_features=132, out_features=164, bias=True)\n",
      "  (out): Linear(in_features=164, out_features=100, bias=True)\n",
      ")\n",
      "F(\n",
      "  (model_phi): Phi(\n",
      "    (lay1): Linear(in_features=3, out_features=4, bias=True)\n",
      "    (lay2): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (lay3): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (out): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      "  (model_rho): Rho(\n",
      "    (lay1): Linear(in_features=100, out_features=132, bias=True)\n",
      "    (lay2): Linear(in_features=132, out_features=164, bias=True)\n",
      "    (out): Linear(in_features=164, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Phi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_size = 3       #len(inputs[0,0,:])\n",
    "        self.hidden1_size = 4\n",
    "        self.hidden2_size = 8\n",
    "        self.hidden3_size = 4\n",
    "        self.out_size = 1\n",
    "        \n",
    "        self.lay1 = nn.Linear(self.in_size,self.hidden1_size)\n",
    "        self.lay2 = nn.Linear(self.hidden1_size,self.hidden2_size)\n",
    "        self.lay3 = nn.Linear(self.hidden2_size,self.hidden3_size)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden3_size,self.out_size)\n",
    "     \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #batch_size, num_particles, space_dim = inputs.shape\n",
    "        inputs = nn.functional.elu(self.lay1(inputs))\n",
    "        inputs = nn.functional.elu(self.lay2(inputs))\n",
    "        inputs = nn.functional.elu(self.lay3(inputs))\n",
    "        out_phi = self.out(inputs)\n",
    "        \n",
    "        return out_phi\n",
    "\n",
    "class Rho(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_size = batch_size #len(inputs[:]) #batch_size\n",
    "        self.hidden1_size = 132\n",
    "        self.hidden2_size = 164\n",
    "        self.out_size = batch_size\n",
    "        \n",
    "        self.lay1 = nn.Linear(self.in_size,self.hidden1_size)\n",
    "        self.lay2 = nn.Linear(self.hidden1_size,self.hidden2_size)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden2_size,self.out_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #batch_size = len(inputs[:])\n",
    "        inputs = nn.functional.elu(self.lay1(inputs))\n",
    "        inputs = nn.functional.elu(self.lay2(inputs))\n",
    "        out_rho = self.out(inputs)\n",
    "        \n",
    "        return out_rho\n",
    "\n",
    "class F(nn.Module):\n",
    "    def __init__(self, Phi, Rho, batch_size):\n",
    "        super().__init__()\n",
    "        self.model_phi = Phi\n",
    "        self.model_rho = Rho\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs is a torch tensor of shape (batch_size, 20, 3)\n",
    "        #batch_size, num_particles, space_dim = inputs.shape\n",
    "        transfer_data = self.model_phi(inputs) # processed through Phi\n",
    "        #performing the summation over the 20 particles\n",
    "        transfer_data = transfer_data.view(batch_size,-1).sum(1) \n",
    "        out = self.model_rho(transfer_data) # processed through Rho\n",
    "        \n",
    "        return out\n",
    "\n",
    "batch_size = 100 # !!!-> NEEDS to be ADAPTED depending on INPUT <-!!!\n",
    "phi = Phi()\n",
    "print(phi)\n",
    "rho = Rho(batch_size) \n",
    "print(rho)\n",
    "f = F(phi,rho,batch_size)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some testing on random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([100, 20, 3])\n",
      "Number of Configurations/ One per batch element: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#defining the data for  test:\n",
    "test_rand = np.random.normal(size=(100, 20, 3))\n",
    "test_data = torch.from_numpy(test_rand.astype(np.float32))\n",
    "print(type(test_data))\n",
    "print(test_data.shape)\n",
    "#applying the permutation invariant network\n",
    "configurations = f(test_data)\n",
    "print('Number of Configurations/ One per batch element:', configurations.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
